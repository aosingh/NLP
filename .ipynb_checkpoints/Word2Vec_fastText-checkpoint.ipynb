{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings v/s fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText embeddings are just an extension of Word2Vec embeddings.\n",
    "\n",
    "fastText also takes into account the morphological nuances while learning the word representations\n",
    "\n",
    "In this notebook, we will try to get an idea of the pros and cons of both the techniques.\n",
    "\n",
    "For testing purposes, we will use the skipgram architecture for the word embeddings.\n",
    "So, from the training perspective, note the following:\n",
    "1. Gensim Word2Vector : skipgram\n",
    "2. fastText : skipgram  + character level n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory called `models/` where we will persist our trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS = 'models/'\n",
    "!mkdir -p {MODELS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "\n",
    "Define the parameters and the method to train.\n",
    "\n",
    "We will compare 3 different methods\n",
    "\n",
    "1. Gensim's Word2Vector embedding\n",
    "2. Facebook's FastText with char n-grams\n",
    "3. FastText without char n-grams.\n",
    "\n",
    "The main purpose is to understand the difference between facebook's fastText and gensim's word2vec.\n",
    "\n",
    "Both of them represent words in vectors; However, fastText also adds char n-grams to the word representation.\n",
    "This is beneficial when we have to take care of the morphological nuances.\n",
    "\n",
    "Simple word2vec tokenizes based on white spaces. \n",
    "\n",
    "The following image explains char-level n-grams\n",
    "\n",
    "<img src=\"./files/29443.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "lr = 0.05\n",
    "dim = 100\n",
    "ws = 5\n",
    "epoch = 5\n",
    "minCount = 5\n",
    "neg = 5\n",
    "loss = 'ns'\n",
    "t = 1e-4\n",
    "\n",
    "params = {\n",
    "    'alpha': lr,\n",
    "    'size': dim,\n",
    "    'window': ws,\n",
    "    'iter': epoch,\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': neg\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def train_models(corpus, output_name):\n",
    "    out_file = '{0}_ft'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS, '{:s}.vec'.format(out_file))):\n",
    "        print('Training fasttext on {:s} corpus..'.format(corpus))\n",
    "        %time !/home/abhishek/fastText/fasttext skipgram -input {corpus} -output {MODELS+out_file} -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n",
    "    else:\n",
    "        print '\\nUsing existing model file {:s}.vec'.format(out_file)\n",
    "\n",
    "    print \"\\n\"\n",
    "    \n",
    "    out_file  = '{0}_ft_no_char_ng'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS, '{:s}.vec'.format(out_file))):\n",
    "        print('Training fasttext with 0 char n-grams on {:s} corpus..'.format(corpus))\n",
    "        %time !/home/abhishek/fastText/fasttext skipgram -input {corpus} -output {MODELS+out_file} -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t} -maxn 0\n",
    "    else:\n",
    "        print '\\nUsing existing model file {:s}.vec'.format(out_file)\n",
    "        \n",
    "    print \"\\n\"\n",
    "        \n",
    "    out_file = '{0}_word2vec'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS, '{:s}.vec'.format(out_file))):\n",
    "        print('Training Gensim word2vector on {:s} corpus..'.format(corpus))\n",
    "        start = time.time()\n",
    "        model = Word2Vec(LineSentence(corpus), **params)\n",
    "        end = time.time()\n",
    "        print \"Time  : \", end - start\n",
    "        model.save(MODELS+out_file+'.vec')\n",
    "    else:\n",
    "        print '\\nUsing existing model file {:s}.vec'.format(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "For training the models, I have used the brown corpus.(can be easily downloaded using nltk python library)\n",
    "\n",
    "Below, you will see the time taken by each of the 3 types of embeddings.\n",
    "\n",
    "As you see below, fastText takes significant amount of time \n",
    "\n",
    "1. fastText (with character level n-grams) 54.5s\n",
    "2. fastText (with out character level n-grams) 19s\n",
    "3. gensim's Word2Vector 17s\n",
    "\n",
    "fastText without character level n-grams should theoritically be same as gensim's Word2Vector.Thus the time taken is almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fasttext on brown_corp.txt corpus..\n",
      "Read 1M words\n",
      "Number of words:  15173\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 50818  lr: 0.000000  loss: 2.395198  eta: 0h0m 4.0%  words/sec/thread: 37170  lr: 0.038002  loss: 2.372056  eta: 0h0m 24.2%  words/sec/thread: 37329  lr: 0.037887  loss: 2.371987  eta: 0h0m 2.378887  eta: 0h0m 33.8%  words/sec/thread: 42972  lr: 0.033103  loss: 2.382141  eta: 0h0m 34.1%  words/sec/thread: 43048  lr: 0.032935  loss: 2.382135  eta: 0h0m 37.9%  words/sec/thread: 44669  lr: 0.031046  loss: 2.384235  eta: 0h0m 38.0%  words/sec/thread: 44786  lr: 0.030976  loss: 2.384403  eta: 0h0m 38.3%  words/sec/thread: 44894  lr: 0.030834  loss: 2.385638  eta: 0h0m 40.1%  words/sec/thread: 45610  lr: 0.029943  loss: 2.388759  eta: 0h0m 40.3%  words/sec/thread: 45636  lr: 0.029846  loss: 2.389348  eta: 0h0m 40.6%  words/sec/thread: 45730  lr: 0.029705  loss: 2.390106  eta: 0h0m 41.0%  words/sec/thread: 45930  lr: 0.029510  loss: 2.391627  eta: 0h0m 44.8%  words/sec/thread: 47158  lr: 0.027621  loss: 2.397135  eta: 0h0m 45.4%  words/sec/thread: 47437  lr: 0.027312  loss: 2.397261  eta: 0h0m 48.3%  words/sec/thread: 48306  lr: 0.025856  loss: 2.403045  eta: 0h0m 48.6%  words/sec/thread: 48424  lr: 0.025679  loss: 2.401446  eta: 0h0m 49.7%  words/sec/thread: 48709  lr: 0.025141  loss: 2.399044  eta: 0h0m 49.9%  words/sec/thread: 48781  lr: 0.025026  loss: 2.399076  eta: 0h0m 53.8%  words/sec/thread: 49840  lr: 0.023102  loss: 2.391072  eta: 0h0m 54.0%  words/sec/thread: 49923  lr: 0.022996  loss: 2.390824  eta: 0h0m 54.3%  words/sec/thread: 49966  lr: 0.022872  loss: 2.390712  eta: 0h0m 54.7%  words/sec/thread: 50105  lr: 0.022652  loss: 2.391182  eta: 0h0m 56.0%  words/sec/thread: 50464  lr: 0.021981  loss: 2.388466  eta: 0h0m 59.3%  words/sec/thread: 51238  lr: 0.020330  loss: 2.388704  eta: 0h0m 59.6%  words/sec/thread: 51289  lr: 0.020215  loss: 2.388075  eta: 0h0m 61.1%  words/sec/thread: 51610  lr: 0.019468  loss: 2.386795  eta: 0h0m 64.4%  words/sec/thread: 52223  lr: 0.017817  loss: 2.385027  eta: 0h0m 64.6%  words/sec/thread: 52297  lr: 0.017702  loss: 2.385019  eta: 0h0m 68.6%  words/sec/thread: 52971  lr: 0.015707  loss: 2.382793  eta: 0h0m %  words/sec/thread: 53020  lr: 0.015592  loss: 2.382024  eta: 0h0m 69.1%  words/sec/thread: 53064  lr: 0.015451  loss: 2.382050  eta: 0h0m 70.6%  words/sec/thread: 53284  lr: 0.014683  loss: 2.380788  eta: 0h0m 71.9%  words/sec/thread: 53465  lr: 0.014048  loss: 2.380545  eta: 0h0m 79.3%  words/sec/thread: 54095  lr: 0.010372  loss: 2.383031  eta: 0h0m 80.5%  words/sec/thread: 54103  lr: 0.009763  loss: 2.382923  eta: 0h0m 81.4%  words/sec/thread: 54112  lr: 0.009295  loss: 2.383915  eta: 0h0m 84.1%  words/sec/thread: 54108  lr: 0.007971  loss: 2.386936  eta: 0h0m 85.5%  words/sec/thread: 54144  lr: 0.007265  loss: 2.386739  eta: 0h0m 89.1%  words/sec/thread: 54166  lr: 0.005464  loss: 2.390041  eta: 0h0m 90.1%  words/sec/thread: 53786  lr: 0.004934  loss: 2.390343  eta: 0h0m 90.3%  words/sec/thread: 53654  lr: 0.004828  loss: 2.390335  eta: 0h0m 90.6%  words/sec/thread: 53380  lr: 0.004687  loss: 2.390139  eta: 0h0m 91.2%  words/sec/thread: 52985  lr: 0.004405  loss: 2.390393  eta: 0h0m 91.8%  words/sec/thread: 52531  lr: 0.004105  loss: 2.390596  eta: 0h0m   words/sec/thread: 52233  lr: 0.003902  loss: 2.391248  eta: 0h0m   words/sec/thread: 50666  lr: 0.000661  loss: 2.394109  eta: 0h0m %  words/sec/thread: 50720  lr: 0.000423  loss: 2.394580  eta: 0h0m \n",
      "CPU times: user 1.63 s, sys: 420 ms, total: 2.05 s\n",
      "Wall time: 54.5 s\n",
      "\n",
      "\n",
      "Training fasttext with 0 char n-grams on brown_corp.txt corpus..\n",
      "Read 1M words\n",
      "Number of words:  15173\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 97836  lr: 0.000000  loss: 2.447498  eta: 0h0m %  words/sec/thread: 7138  lr: 0.049020  loss: 3.214782  eta: 0h1m 3.4%  words/sec/thread: 12011  lr: 0.048288  loss: 2.828896  eta: 0h0m 4.8%  words/sec/thread: 15812  lr: 0.047581  loss: 2.700425  eta: 0h0m 9.6%  words/sec/thread: 28210  lr: 0.045198  loss: 2.591386  eta: 0h0m 10.1%  words/sec/thread: 29379  lr: 0.044933  loss: 2.582082  eta: 0h0m 11.4%  words/sec/thread: 32249  lr: 0.044324  loss: 2.572573  eta: 0h0m 18.1%  words/sec/thread: 44417  lr: 0.040961  loss: 2.549031  eta: 0h0m %  words/sec/thread: 46507  lr: 0.040361  loss: 2.539581  eta: 0h0m 19.7%  words/sec/thread: 47075  lr: 0.040147  loss: 2.539671  eta: 0h0m 21.9%  words/sec/thread: 50224  lr: 0.039070  loss: 2.531252  eta: 0h0m 22.3%  words/sec/thread: 50755  lr: 0.038867  loss: 2.529299  eta: 0h0m 23.1%  words/sec/thread: 52073  lr: 0.038452  loss: 2.519768  eta: 0h0m 27.3%  words/sec/thread: 57506  lr: 0.036369  loss: 2.505906  eta: 0h0m 27.7%  words/sec/thread: 57934  lr: 0.036131  loss: 2.503871  eta: 0h0m 28.1%  words/sec/thread: 58448  lr: 0.035937  loss: 2.500923  eta: 0h0m 28.5%  words/sec/thread: 58790  lr: 0.035734  loss: 2.501304  eta: 0h0m 30.6%  words/sec/thread: 61063  lr: 0.034683  loss: 2.497410  eta: 0h0m 31.1%  words/sec/thread: 61624  lr: 0.034462  loss: 2.495524  eta: 0h0m 35.8%  words/sec/thread: 66222  lr: 0.032106  loss: 2.490627  eta: 0h0m 36.2%  words/sec/thread: 66728  lr: 0.031876  loss: 2.490982  eta: 0h0m 39.3%  words/sec/thread: 69469  lr: 0.030349  loss: 2.492189  eta: 0h0m 39.7%  words/sec/thread: 69734  lr: 0.030137  loss: 2.491895  eta: 0h0m 44.7%  words/sec/thread: 73743  lr: 0.027648  loss: 2.493494  eta: 0h0m 45.2%  words/sec/thread: 74039  lr: 0.027418  loss: 2.491238  eta: 0h0m 45.6%  words/sec/thread: 74376  lr: 0.027189  loss: 2.489301  eta: 0h0m 46.2%  words/sec/thread: 74920  lr: 0.026924  loss: 2.487521  eta: 0h0m 47.3%  words/sec/thread: 75589  lr: 0.026350  loss: 2.484964  eta: 0h0m 50.8%  words/sec/thread: 77951  lr: 0.024576  loss: 2.474229  eta: 0h0m 51.2%  words/sec/thread: 78163  lr: 0.024408  loss: 2.473289  eta: 0h0m 53.5%  words/sec/thread: 79622  lr: 0.023261  loss: 2.469759  eta: 0h0m 57.6%  words/sec/thread: 82013  lr: 0.021204  loss: 2.468396  eta: 0h0m 59.1%  words/sec/thread: 83008  lr: 0.020427  loss: 2.468281  eta: 0h0m 60.1%  words/sec/thread: 83402  lr: 0.019944  loss: 2.465507  eta: 0h0m 63.5%  words/sec/thread: 85099  lr: 0.018258  loss: 2.461125  eta: 0h0m 64.7%  words/sec/thread: 85721  lr: 0.017658  loss: 2.460413  eta: 0h0m 2.456666  eta: 0h0m 74.6%  words/sec/thread: 90009  lr: 0.012679  loss: 2.450569  eta: 0h0m 78.7%  words/sec/thread: 91719  lr: 0.010667  loss: 2.449909  eta: 0h0m 80.5%  words/sec/thread: 92297  lr: 0.009731  loss: 2.450318  eta: 0h0m 82.5%  words/sec/thread: 93043  lr: 0.008757  loss: 2.450594  eta: 0h0m 84.2%  words/sec/thread: 93600  lr: 0.007891  loss: 2.450050  eta: 0h0m 84.6%  words/sec/thread: 93762  lr: 0.007706  loss: 2.450310  eta: 0h0m 85.5%  words/sec/thread: 94008  lr: 0.007265  loss: 2.450947  eta: 0h0m 87.6%  words/sec/thread: 94683  lr: 0.006214  loss: 2.450235  eta: 0h0m 91.7%  words/sec/thread: 95892  lr: 0.004131  loss: 2.449487  eta: 0h0m 93.2%  words/sec/thread: 96311  lr: 0.003380  loss: 2.449464  eta: 0h0m 98.6%  words/sec/thread: 97509  lr: 0.000705  loss: 2.448179  eta: 0h0m 99.4%  words/sec/thread: 97695  lr: 0.000291  loss: 2.447789  eta: 0h0m 2.447498  eta: 0h0m \n",
      "CPU times: user 752 ms, sys: 124 ms, total: 876 ms\n",
      "Wall time: 19 s\n",
      "\n",
      "\n",
      "Training Gensim word2vector on brown_corp.txt corpus..\n",
      "Time  :  17.2614369392\n"
     ]
    }
   ],
   "source": [
    "train_models('brown_corp.txt', 'brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results : \n",
    "\n",
    "To test the performance of the word embeddings, we will use the 'questions-words.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-16 12:38:50--  https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.64.133, 151.101.128.133, 151.101.192.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.64.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 603955 (590K) [text/plain]\n",
      "Saving to: ‘questions-words.txt’\n",
      "\n",
      "questions-words.txt 100%[===================>] 589.80K  1.96MB/s    in 0.3s    \n",
      "\n",
      "2017-05-16 12:38:51 (1.96 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Scores \n",
    "\n",
    "The accuracy scores have been reported based on semantic and syntactic tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "def print_accuracy(model, questions_file):\n",
    "    print('Evaluating...\\n')\n",
    "    acc = model.accuracy(questions_file)\n",
    "\n",
    "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
    "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
    "    sem_acc = 100*float(sem_correct)/sem_total\n",
    "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
    "    \n",
    "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
    "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
    "    syn_acc = 100*float(syn_correct)/syn_total\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
    "    return (sem_acc, syn_acc)\n",
    "\n",
    "word_analogies_file = 'questions-words.txt'\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Word2Vec(Gensim): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Gensim embeddings\n",
      "Accuracy for Word2Vec:\n",
      "Evaluating...\n",
      "\n",
      "\n",
      "Semantic: 30/813, Accuracy: 3.69%\n",
      "Syntactic: 97/5255, Accuracy: 1.85%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading Gensim embeddings')\n",
    "myCorpus_gs = Word2Vec.load(MODELS + 'brown_word2vec.vec')\n",
    "print('Accuracy for Word2Vec:')\n",
    "accuracies.append(print_accuracy(myCorpus_gs, word_analogies_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for FastText (with n-grams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FastText embeddings\n",
      "Accuracy for FastText (with n-grams):\n",
      "Evaluating...\n",
      "\n",
      "\n",
      "Semantic: 47/813, Accuracy: 5.78%\n",
      "Syntactic: 2629/5255, Accuracy: 50.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading FastText embeddings')\n",
    "ft_model = gensim.models.KeyedVectors.load_word2vec_format(MODELS + 'brown_ft.vec')\n",
    "print('Accuracy for FastText (with n-grams):')\n",
    "accuracies.append(print_accuracy(ft_model, word_analogies_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for FastText (without char n-grams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FastText embeddings without char n-grams\n",
      "Accuracy for FastText (without char n-grams):\n",
      "Evaluating...\n",
      "\n",
      "\n",
      "Semantic: 43/813, Accuracy: 5.29%\n",
      "Syntactic: 119/5255, Accuracy: 2.26%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading FastText embeddings without char n-grams')\n",
    "ft_model = gensim.models.KeyedVectors.load_word2vec_format(MODELS + 'brown_ft_no_char_ng.vec')\n",
    "print('Accuracy for FastText (without char n-grams):')\n",
    "accuracies.append(print_accuracy(ft_model, word_analogies_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary : \n",
    "\n",
    "We see that, for syntactic tasks, `fastText`(with character level  n-grams) has performed significantly better than gensim's word2vec. \n",
    "\n",
    "The results for semantic tasks are pretty much the same for both `word2vec` and `fastText`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
